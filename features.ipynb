{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('features/X_train_features.csv')\n",
    "y_train = pd.read_csv('public/y_train.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(5117, 2247) (5117, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Imput missing values using the mean of each column (basic : try to find more pertinent)\n",
    "\n",
    "# imput missing values using the k-neighbors imputer (more advanced)\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# Create the imputer object, with 10 neighbors\n",
    "imputer = KNNImputer(n_neighbors=10, weights='distance')\n",
    "\n",
    "# Fit the imputer object on the train data\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Impute the missing values on the train and test data\n",
    "X_train = imputer.transform(X_train)\n",
    "\n",
    "# Check that there is no more missing values    \n",
    "print(np.isnan(X_train).sum())\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5117, 1813) (5117, 2)\n"
     ]
    }
   ],
   "source": [
    "# Remove features with low variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.1)  # remove features with more than 80% variance\n",
    "X_train = sel.fit_transform(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant features :  685\n",
      "[0, 1, 8, 9, 15, 17, 19, 20, 22, 23, 24, 25, 26, 28, 29, 30, 37, 40, 41, 44, 45, 47, 54, 57, 58, 59, 62, 64, 65, 72, 75, 76, 80, 82, 84, 86, 88, 89, 92, 93, 94, 97, 118, 120, 122, 123, 126, 130, 132, 135, 153, 160, 164, 167, 169, 175, 186, 187, 188, 191, 200, 203, 204, 206, 210, 213, 217, 223, 227, 234, 240, 244, 247, 257, 261, 268, 273, 274, 276, 278, 281, 285, 291, 294, 296, 298, 303, 304, 307, 309, 311, 315, 316, 317, 321, 322, 323, 326, 329, 330, 331, 332, 333, 334, 340, 341, 344, 345, 346, 347, 349, 354, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 374, 375, 377, 378, 381, 387, 389, 390, 391, 392, 393, 396, 402, 404, 405, 410, 416, 424, 430, 432, 440, 444, 452, 456, 460, 468, 469, 470, 471, 472, 474, 475, 476, 478, 489, 490, 491, 492, 493, 495, 496, 510, 511, 512, 514, 515, 516, 518, 519, 520, 522, 523, 529, 530, 531, 533, 534, 537, 538, 541, 542, 549, 550, 551, 553, 559, 562, 570, 576, 577, 588, 590, 597, 598, 600, 609, 610, 612, 615, 619, 620, 621, 622, 623, 626, 629, 640, 645, 649, 650, 651, 653, 657, 658, 662, 663, 666, 667, 668, 671, 683, 705, 707, 713, 714, 717, 718, 721, 722, 725, 726, 728, 730, 731, 732, 734, 735, 738, 739, 742, 743, 747, 748, 751, 752, 755, 756, 759, 760, 764, 765, 773, 776, 777, 781, 785, 786, 787, 789, 793, 794, 795, 798, 799, 800, 803, 804, 805, 808, 811, 812, 813, 816, 817, 820, 821, 822, 830, 833, 834, 842, 846, 847, 851, 852, 853, 855, 856, 859, 863, 864, 865, 869, 870, 872, 873, 880, 881, 882, 885, 886, 887, 890, 891, 893, 894, 897, 898, 899, 902, 903, 904, 911, 914, 915, 919, 923, 924, 925, 927, 931, 932, 933, 936, 937, 938, 941, 942, 943, 946, 949, 950, 951, 954, 955, 958, 959, 960, 968, 971, 972, 977, 978, 983, 984, 988, 992, 993, 1000, 1001, 1002, 1005, 1006, 1007, 1008, 1010, 1011, 1012, 1018, 1019, 1020, 1023, 1024, 1025, 1026, 1028, 1029, 1030, 1031, 1032, 1033, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1115, 1118, 1119, 1120, 1123, 1128, 1129, 1130, 1131, 1132, 1136, 1137, 1138, 1141, 1142, 1143, 1145, 1146, 1150, 1153, 1154, 1155, 1158, 1159, 1160, 1162, 1163, 1167, 1170, 1171, 1172, 1176, 1180, 1184, 1187, 1189, 1192, 1201, 1202, 1205, 1206, 1207, 1210, 1211, 1212, 1213, 1215, 1216, 1223, 1225, 1229, 1233, 1240, 1242, 1246, 1254, 1257, 1259, 1262, 1264, 1267, 1268, 1269, 1271, 1275, 1276, 1277, 1281, 1282, 1284, 1285, 1286, 1289, 1292, 1293, 1294, 1298, 1299, 1301, 1305, 1306, 1309, 1310, 1311, 1315, 1316, 1323, 1326, 1328, 1330, 1331, 1340, 1341, 1344, 1345, 1346, 1349, 1350, 1351, 1352, 1354, 1355, 1362, 1364, 1368, 1372, 1379, 1381, 1385, 1390, 1393, 1398, 1401, 1404, 1407, 1408, 1410, 1411, 1413, 1414, 1415, 1416, 1419, 1421, 1422, 1424, 1425, 1426, 1432, 1434, 1438, 1441, 1442, 1446, 1451, 1454, 1456, 1459, 1460, 1461, 1463, 1472, 1480, 1482, 1487, 1489, 1493, 1494, 1496, 1497, 1498, 1499, 1500, 1501, 1504, 1505, 1506, 1510, 1511, 1513, 1514, 1516, 1521, 1522, 1523, 1527, 1528, 1530, 1531, 1538, 1540, 1543, 1544, 1545, 1546, 1548, 1549, 1553, 1556, 1557, 1558, 1561, 1562, 1563, 1564, 1566, 1567, 1570, 1574, 1575, 1576, 1579, 1580, 1581, 1582, 1584, 1585, 1592, 1593, 1594, 1597, 1598, 1602, 1606, 1609, 1611, 1618, 1619, 1621, 1626, 1627, 1628, 1631, 1632, 1633, 1635, 1636, 1643, 1644, 1645, 1649, 1650, 1652, 1653, 1660, 1661, 1662, 1666, 1667, 1669, 1670, 1677, 1679, 1682, 1683, 1684, 1685, 1687, 1688, 1692, 1695, 1696, 1697, 1700, 1701, 1702, 1703, 1705, 1706, 1709, 1713, 1714, 1715, 1718, 1719, 1720, 1721, 1723, 1724, 1731, 1732, 1733, 1736, 1737, 1738, 1746, 1749, 1750, 1751, 1754, 1755, 1756, 1757, 1759, 1760, 1767, 1768, 1769, 1772, 1773, 1774, 1775, 1777, 1778, 1780, 1785, 1786, 1787, 1790, 1791, 1792, 1793, 1795, 1796, 1803, 1804, 1805, 1808, 1809, 1810, 1811]\n"
     ]
    }
   ],
   "source": [
    "### Drop highly correlated features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming that X_train is your ndarray and it only contains feature columns\n",
    "df = pd.DataFrame(X_train)\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a set to hold the correlated columns\n",
    "corr_columns = set()\n",
    "\n",
    "# Iterate over the correlation matrix\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        # If the correlation between the columns is high, add it to the set\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            corr_columns.add(colname)\n",
    "\n",
    "# Get the indices of the relevant features\n",
    "relevant_features = [df.columns.get_loc(c) for c in df.columns if c not in corr_columns]\n",
    "\n",
    "X_train = X_train[:, relevant_features]\n",
    "# Print the relevant feature indices\n",
    "print(\"Number of relevant features : \", X_train.shape[1])\n",
    "print(relevant_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('public/y_train.csv', index_col='id')\n",
    "y_train = y_train.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5117, 300)\n"
     ]
    }
   ],
   "source": [
    "# Select the most relevant features\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Create the SelectKBest with the mutual info strategy\n",
    "selector = SelectKBest(f_regression, k=300)\n",
    "\n",
    "# Fit the object to the training data\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train = selector.transform(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.read_csv('public/y_train.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, index=y_df.index)\n",
    "X_train_df.to_csv('features/X_train_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amltask2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
